{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVmT47LOBy6i"
      },
      "source": [
        " <center> <h1> <b> Pattern Recognition and Machine Learning (EE5610 - EE2802 - AI2000 - AI5000) </b> </h1> </center>\n",
        "\n",
        "<b> Programming Assignment - 03 : Classification</b>\n",
        "\n",
        "\n",
        "This programming assignment offers you an opportunity to implement various linear classification models. You'll begin with straightforward algorithms like Least Squares classification, LDA, and FDA. Subsequently, you'll delve into implementing the perceptron algorithm. Lastly, you'll tackle probabilistic approaches for classification. To deepen your understanding of these methods, you'll compare their performance and application scenarios critically.\n",
        "\n",
        "<b> Instructions </b>\n",
        "1. Plagiarism is strictly prohibited.\n",
        "2. Delayed submissions will be penalized with a scaling factor of 0.5 per day.\n",
        "3. Please DO NOT use any machine learning libraries unless and otherwise specified.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part-1) Least squares approach to classification**\n",
        "\n",
        "a). Data generation: Consider a classification scenario with two classes. Class 1 follows a Gaussian distribution with a mean vector $\\begin{bmatrix} 1  \\\\ 1  \\end{bmatrix}$ and a covariance matrix $\\begin{bmatrix} 0.3 & 0.0  \\\\ 0.0 & 0.3 \\end{bmatrix}$. Class 2 follows a Gaussian distribution with a mean vector $\\begin{bmatrix} 2  \\\\ 2  \\end{bmatrix}$ and a covariance matrix $\\begin{bmatrix} 0.3 & 0.0  \\\\ 0.0 & 0.3 \\end{bmatrix}$. Employ these class distributions to generate 50 samples per class for training and 200 samples for testing.\n",
        "\n",
        "b). Find the decision boundary: Assign the target value \"0\" to all data points in class 1 and the target value \"1\" to all data points in class 2. Implement the pseudo-inverse solution to determine the weight vector, which represents the decision boundary.\n",
        "\n",
        "\n",
        "c). Evaluate the quality of decision boundary: A decision boundary is deemed effective if it correctly classifies all data points. Accuracy measures the performance of a classification problem. Calculate the accuracy of the test data using the decision boundary obtained from the pseudo-inverse solution.\n",
        "\n",
        "\n",
        "d). Visualize the decision boundary: You can either display the decision hyperplane or use different colors to distinguish regions corresponding to the two classes on a 2D plane. Superimpose the test points onto the decision boundary plot.\n",
        "\n",
        "e). Assign the value \"-1\" to class1 and the value \"+1\" to class 2. Repeat the experiments.\n",
        "\n",
        "f). Experiment with Laplacian distribution and compare the performance of it with the Gaussian distribution experiments\n",
        "\n",
        "g. Report your observations.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hkEmZEo8E2fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#All imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import random\n",
        "import math"
      ],
      "metadata": {
        "id": "m7J5ASpGERVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########################################\n",
        "#Part-1)-a: Data generation\n",
        "########################################\n",
        "\n",
        "#Set the parameters of class distribution\n",
        "mean1 = [1,1]           #Mean of class1\n",
        "mean2 = [2,2]           #Mean of class2\n",
        "cov = [[0.3,0],[0,0.3]] #Covariance matrix. Same covariance for class1 and class2\n",
        "\n",
        "#Generate training data. You can use \"np.random.multivariate_normal()\" function to sample data points from multivariate Gaussian distribution\n",
        "class1_tr =             #Class1 training data\n",
        "class2_tr =             #Class2 training data\n",
        "tr_data =               #Combine class1 training and class2 training data to get total training data\n",
        "tr_targets =            #Class labels. Assign the value \"0\" to class1 and the value \"1\" to class2.\n",
        "\n",
        "#Generate testing data.\n",
        "class2_te =             #Class1 testing data\n",
        "class2_te =             #Class2 testing data\n",
        "te_data =               #Combine class1 testing and class2 testing data to get total testing data\n",
        "te_targets =            #Class labels. Assign the value \"0\" to class1 and the value \"1\" to class2.\n",
        "\n",
        "\n",
        "########################################\n",
        "#Part-1)-b: Find the decision boundary\n",
        "########################################\n",
        "#Complete the below function\n",
        "def LS_Classify(X_train, Y_train, X_test):\n",
        "    #Inputs: Training data, Training labels, and Testing data\n",
        "    #Outputs: Testing labels\n",
        "\n",
        "    #Impliment pseudo inverse solution to get the weight vector\n",
        "\n",
        "\n",
        "\n",
        "    #Predict the lables of test data using the pseudo inverse solution\n",
        "\n",
        "\n",
        "\n",
        "    #Return the predicted test labels\n",
        "\n",
        "#Get the test data predictions\n",
        "predictions = LS_Classify(tr_data, tr_targets, te_data)\n",
        "\n",
        "\n",
        "##########################################\n",
        "#Part-1)-c: Evaluate the quality of decision boundary\n",
        "##########################################\n",
        "#Complete the below function\n",
        "def LS_Classify_Accuracy(Y_test, Y_pred):\n",
        "    #Inputs: Ground truth test labels and predicted test labels\n",
        "    #Outputs: Accuracy\n",
        "\n",
        "    #Compute the accuracy\n",
        "\n",
        "\n",
        "    #Return the accuracy\n",
        "\n",
        "\n",
        "\n",
        "##########################################\n",
        "#Part-1)-d: Visualize the decision boundary\n",
        "##########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########################################\n",
        "#Part-1)-e: Change the target label notation, and repeat the experiments\n",
        "##########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########################################\n",
        "#Part-1)-f: Repeat the experiments with Laplacial distribution\n",
        "##########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "81_5YbU8dpes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Report your observations </b>\n",
        "\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "3.\n",
        "\n"
      ],
      "metadata": {
        "id": "T8NY_9Lf8QEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<b> Part - (2) :  Linear & Fisher Discriminant analysis : </b> In this segment of the programming task, you'll learn the technique of projecting data from higher-dimensional to lower-dimensional space using both Linear Discriminant Analysis and Fisher Discriminant Analysis.\n",
        "\n",
        "a). Data generation: Consider a classification scenario with two classes. Class 1 follows a Gaussian distribution with a mean vector $\\begin{bmatrix} 1  \\\\ 1  \\end{bmatrix}$ and a covariance matrix $\\begin{bmatrix} 0.3 & 0.0  \\\\ 0.0 & 0.3 \\end{bmatrix}$. Class 2 follows a Gaussian distribution with a mean vector $\\begin{bmatrix} 2  \\\\ 2  \\end{bmatrix}$ and a covariance matrix $\\begin{bmatrix} 0.3 & 0.0  \\\\ 0.0 & 0.3 \\end{bmatrix}$. Employ these class distributions to generate 50 samples per class for training and 200 samples for testing.\n",
        "\n",
        "b). Projection using only the between-class covariance: Develop a function for performing projection from 2-D space to 1-D utilizing Linear Discriminant Analysis. It's important to emphasize that this method only considers the between-class covariance. Project the previously generated 2-D synthetic data into 1-D and conduct classification. Plot both the original and projected data points on the same graph.\n",
        "\n",
        "c). Projection utilizing both between-class and within-class covariance: Create a function to perform projection from 2-D space to 1-D using Fisher Discriminant Analysis. It's essential to mention that this method considers both between-class covariance and within-class covariance. Project the previously generated 2-D synthetic data into 1-D and conduct classification. Plot both the original and projected data points on the same graph.\n",
        "\n",
        "d). Extend Fisher discriminant analysis function to project the data from N dimenision to K dimension.\n",
        "\n",
        "e). Report your observations.\n"
      ],
      "metadata": {
        "id": "etb5RPr5_qAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#Part-2)-(a): Data generation\n",
        "########################################\n",
        "\n",
        "#Set the parameters of class distribution\n",
        "mean1 = [1,1]           #Mean of class1\n",
        "mean2 = [2,2]           #Mean of class2\n",
        "cov = [[0.3,0],[0,0.3]] #Covariance matrix. Same covariance for class1 and class2\n",
        "\n",
        "#Generate training data. You can use \"np.random.multivariate_normal()\" function to sample data points from multivariate Gaussian distribution\n",
        "class1_tr =             #Class1 training data\n",
        "class2_tr =             #Class2 training data\n",
        "tr_data =               #Combine class1 training and class2 training data to get total training data\n",
        "tr_targets =            #Class labels. Assign the value \"0\" to class1 and the value \"1\" to class2.\n",
        "\n",
        "#Generate testing data.\n",
        "class2_te =             #Class1 testing data\n",
        "class2_te =             #Class2 testing data\n",
        "te_data =               #Combine class1 testing and class2 testing data to get total testing data\n",
        "te_targets =            #Class labels. Assign the value \"0\" to class1 and the value \"1\" to class2.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Part-2)-(b)\n",
        "########################################\n",
        "#Complete the below function\n",
        "def LDA_classify(X_train, Y_train, X_test):\n",
        "  #Inputs: Training data, Training labels, and Testing data\n",
        "  #Outputs: Between class covariance\n",
        "\n",
        "  #Compute the weight vector\n",
        "\n",
        "\n",
        "\n",
        "  #Compute the threshold\n",
        "\n",
        "\n",
        "\n",
        "  #Project the data onto 1 dimensional space. Compare it with the threshold to make the decision\n",
        "\n",
        "\n",
        "\n",
        "  #Return weight vector and the predictions on testing data\n",
        "\n",
        "  return predictions, w\n",
        "\n",
        "predictions, w = LDA_classify(tr_data, tr_targets, te_data)\n",
        "\n",
        "\n",
        "def LDA_project(w, X_test, Y_test):\n",
        "  #Inputs: weight vector, testing data, and testing labels\n",
        "\n",
        "  #Project the test data onto 1-dimensional space\n",
        "\n",
        "\n",
        "  #Plot the projections\n",
        "\n",
        "\n",
        "LDA_project(w, te_data, te_targets)\n",
        "#Compute and print the accuracy\n",
        "print('Accuracy with LDA:', acc*100)\n",
        "\n",
        "\n",
        "########################################\n",
        "#Part-2)-(c)\n",
        "########################################\n",
        "def FDA_classify(X_train, Y_train, X_test):\n",
        "  #Inputs: Training data, Training labels, and Testing data\n",
        "  #Outputs: Weight vector and predictions\n",
        "\n",
        "  #Compute the weight vector\n",
        "\n",
        "\n",
        "\n",
        "  #Compute the threshold\n",
        "\n",
        "\n",
        "\n",
        "  #Project the data onto 1 dimensional space. Compare it with the threshold to make the decision\n",
        "\n",
        "\n",
        "\n",
        "  #Return weight vector and the predictions on testing data\n",
        "\n",
        "  return w, predictions\n",
        "\n",
        "w, predictions = FDA_classify(tr_data, tr_targets, te_data)\n",
        "\n",
        "\n",
        "def FDA_project(w, X_test, Y_test):\n",
        "  #Inputs: weight vector, testing data, and testing labels\n",
        "\n",
        "  #Project the test data onto 1-dimensional space\n",
        "\n",
        "\n",
        "  #Plot the projections\n",
        "\n",
        "\n",
        "FDA_project(w, te_data, te_targets)\n",
        "#Compute and print the accuracy\n",
        "print('Accuracy with FDA:', acc*100)\n",
        "\n",
        "########################################\n",
        "#Part-2)-(d)\n",
        "########################################\n",
        "def FDA_project_K(X_train, Y_train, X_test, k):\n",
        "  #Inputs: Training data, Training label, Testing data, and the number of eigen vectors to be considered\n",
        "  #Oupur: Projection matrix\n",
        "\n",
        "\n",
        "  #Compute and return the projection matrix\n",
        "  return w\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c8gxhAoTdvma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Report your observations </b>\n",
        "\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "3.\n",
        "\n"
      ],
      "metadata": {
        "id": "KFfifc-c5wqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Part (3) : Classification using perceptron algorithm </b>\n",
        "\n",
        "a). Data generation: Consider a classification scenario with two classes. Class 1 follows a Gaussian distribution with a mean vector $\\begin{bmatrix} 1  \\\\ 1  \\end{bmatrix}$ and a covariance matrix $\\begin{bmatrix} 0.3 & 0.0  \\\\ 0.0 & 0.3 \\end{bmatrix}$. Class 2 follows a Gaussian distribution with a mean vector $\\begin{bmatrix} 2  \\\\ 2  \\end{bmatrix}$ and a covariance matrix $\\begin{bmatrix} 0.3 & 0.0  \\\\ 0.0 & 0.3 \\end{bmatrix}$. Employ these class distributions to generate 50 samples per class for training and 200 samples for testing.\n",
        "\n",
        "\n",
        "b). Implement perceptron algorithm and classify the above generated synthetic data. Plot the decision boundary/regions.\n",
        "\n",
        "c). Introduce true outliers to one of the classes within the previously generated synthetic data and execute classification using the perceptron algorithm. Illustrate the decision boundary/regions. Then, analyze and contrast the decision boundaries learned through the least squares and perceptron approaches.\n",
        "\n",
        "d). Let us consider four Gaussian distributions with mean vectors as $\\begin{bmatrix} 0  \\\\ 0  \\end{bmatrix}$, $\\begin{bmatrix} 0  \\\\ 1  \\end{bmatrix}$, $\\begin{bmatrix} 1  \\\\ 1  \\end{bmatrix}$, and $\\begin{bmatrix} 1  \\\\ 0  \\end{bmatrix}$, respectively. The covariance matrix is the same for all four Gaussian distributions, and the matrix is $\\begin{bmatrix} 0.3 & 0.0  \\\\ 0.0 & 0.3 \\end{bmatrix}$. Sample 60 data points from each distribution to get 240 data points. Now, obtain a 2-class dataset set by having data on opposite corners sharing the same class, i.e., data points sampled from Gaussian distributions with mean vectors $\\begin{bmatrix} 0  \\\\ 0  \\end{bmatrix}$ and $\\begin{bmatrix} 1  \\\\ 1  \\end{bmatrix}$ belong to class 1, and the data points sampled from the other two distributions belong to class 2. Assign class 1 data points with the label +1 and class 2 with the label -1. Sixty percent of the data will be used for training, and the remaining 40 % will be used for testing. You can see that it represents the XOR problem. Classify this data set using the perceptron algorithm.\n",
        "  \n",
        "\n",
        "e). Report your observations\n"
      ],
      "metadata": {
        "id": "l1505Kdf2sUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#Part-3)-(a): Data generation\n",
        "########################################\n",
        "\n",
        "#Set the parameters of class distribution\n",
        "mean1 = [1,1]           #Mean of class1\n",
        "mean2 = [2,2]           #Mean of class2\n",
        "cov = [[0.3,0],[0,0.3]] #Covariance matrix. Same covariance for class1 and class2\n",
        "\n",
        "#Generate training data. You can use \"np.random.multivariate_normal()\" function to sample data points from multivariate Gaussian distribution\n",
        "class1_tr =             #Class1 training data\n",
        "class2_tr =             #Class2 training data\n",
        "tr_data =               #Combine class1 training and class2 training data to get total training data\n",
        "tr_targets =            #Class labels. Assign the value \"0\" to class1 and the value \"1\" to class2.\n",
        "\n",
        "#Generate testing data.\n",
        "class2_te =             #Class1 testing data\n",
        "class2_te =             #Class2 testing data\n",
        "te_data =               #Combine class1 testing and class2 testing data to get total testing data\n",
        "te_targets =            #Class labels. Assign the value \"0\" to class1 and the value \"1\" to class2.\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Part-3)-(b)\n",
        "########################################\n",
        "def Perceptron(X_train, Y_train, X_test):\n",
        "    #Inputs: Training data, Training labels, Testing data\n",
        "    #Outputs: Testing data predictions, Weight vector representing the decision boundary\n",
        "\n",
        "    epochs =     #Number of epochs\n",
        "    X =          #Data. Append ones to the training data to take care of the bias\n",
        "    w =          #Initialize the weight vector\n",
        "    for epoch in range(0,epochs,1):\n",
        "        #Compute the predictions\n",
        "\n",
        "\n",
        "        #Compute the error\n",
        "\n",
        "\n",
        "        #Update the weight vector\n",
        "\n",
        "\n",
        "    #Compute the test data predicitions using the final weight vector\n",
        "\n",
        "\n",
        "\n",
        "    #Return the test data predictions and the final weight vector\n",
        "    return predictions, w\n",
        "\n",
        "\n",
        "##########################################\n",
        "#Plot the decision boundary using perceptron\n",
        "##########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########################################\n",
        "#Plot the decision boundary using least squares\n",
        "##########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########################################\n",
        "#Part-3)-(c): Repeat the experiments with the true outliers added to the data. Compare and contrast the perceptron algorithm with least squares method\n",
        "##########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########################################\n",
        "#Part-3)-(d): Data generation XOR\n",
        "##########################################\n",
        "\n",
        "#Set the parameters of Gaussian distribution\n",
        "class1_mean1 =      #Mean of Gaussian1\n",
        "class1_mean2 =      #Mean of Gaussian3\n",
        "class2_mean1 =      #Mean of Gaussian2\n",
        "class2_mean2 =      #Mean of Gaussian4\n",
        "cov =   #Covariance matrix. Same covariance for class1 and class2\n",
        "\n",
        "#Generate  data. You can use \"np.random.multivariate_normal()\" function to sample data points from multivariate Gaussian distribution\n",
        "\n",
        "class1_data1 =\n",
        "class1_data2 =\n",
        "class2_data1 =\n",
        "class2_data2 =\n",
        "\n",
        "#Obtain 2-class dataset\n",
        "\n",
        "\n",
        "\n",
        "#Create train test splits\n",
        "\n",
        "\n",
        "\n",
        "#Classify this dataset using perceptron algorithm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Plot the decision boundary\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gByCS1JDd1ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Report your observations </b>\n",
        "\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "3.\n",
        "\n"
      ],
      "metadata": {
        "id": "Im4aFdk5TJcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Part (4) : Understanding the decision boundaries of MAP approach to classification - </b> Decision boundary in 2 class classification problem is the locus of points satisfying $p(c_{1}/x)$ = $p(c_{2}/x)$. Where $c_{1}$ and $c_{2}$ are the class indicators and $x$ is the data. With proper assumpuptions on prior and likelihood, p(c/x) follows Gaussian distribution with appropriate parameters. Given the parameters of the Guassian distribution for the two classes i.e $\\mu_{1},\\Sigma_{1},\\mu_{2},\\Sigma_{2}$, we can derive the decision boundary equation i.e W and $w_{0}$. You may refer to class notes for the derivations and final equations. In this part of programming assignment we expect you to code the decision boundaries for the Gaussian distribution case and understand them.\n",
        "\n",
        "**a). Class distributions share the same covariance matrix: Linear decision boundary.**\n",
        "  0. Let $\\mu_{1} = \\begin{bmatrix} 1  \\\\ 1 \\end{bmatrix}$, $\\mu_{2} = \\begin{bmatrix} 4  \\\\ 1 \\end{bmatrix}$, and $\\Sigma_{1}= \\Sigma_{2} = \\Sigma =  \\begin{bmatrix} 0.1 & 0.0  \\\\ 0.0 & 0.1 \\end{bmatrix}$\n",
        "  1. Generate 100 samples from each class. This data set is used to understand the decision boundaries when $p(c_{1}) = p(c_{2})$\n",
        "  2. Generate 50 samples from class1 and 150 samples from class2. This data set is used to understand the decision boundaries when $p(c_{1}) < p(c_{2})$\n",
        "  3. Generate 150 samples from class1 and 50 samples from class2. This data set is used to understand the decision boundaries when $p(c_{1}) > p(c_{2})$\n",
        "  4. Plot the decision boundaries for all the three cases in three different subplots.\n",
        "\n",
        "\n",
        "**b). Class distributions have different covariance matrices: Non-linear decision boundary.**\n",
        "  0. Let $\\mu_{1} = \\begin{bmatrix} 1  \\\\ 1 \\end{bmatrix}$, $\\mu_{2} = \\begin{bmatrix} 3  \\\\ 1 \\end{bmatrix}$, $\\Sigma_{1}=   \\begin{bmatrix} 0.2 & 0.0  \\\\ 0.0 & 0.02 \\end{bmatrix}$, and $\\Sigma_{2}=   \\begin{bmatrix} 0.02 & 0.0  \\\\ 0.0 & 0.2 \\end{bmatrix}$\n",
        "  1. Generate 100 samples from each class. This data set is used to understand the decision boundaries when $p(c_{1}) = p(c_{2})$\n",
        "  2. Generate 50 samples from class1 and 150 samples from class2. This data set is used to understand the decision boundaries when $p(c_{1}) < p(c_{2})$\n",
        "  3. Generate 150 samples from class1 and 50 samples from class2. This data set is used to understand the decision boundaries when $p(c_{1}) > p(c_{2})$\n",
        "  4. Plot the decision boundaries for all the three cases in three different subplots.\n",
        "\n",
        "\n",
        "**c). Report your observations**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r1Xvw64k25Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################\n",
        "#Part-4a)-0: Data Generation\n",
        "##########################################\n",
        "m1 =\n",
        "m2 =\n",
        "cov =\n",
        "\n",
        "#Case1: Generate 100 samples from each class. You may use \"np.random.multivariate_normal()\" function to sample data points of multi-variate Gaussian distribution\n",
        "\n",
        "\n",
        "\n",
        "#Case2: Generate 50 samples from class1 and 150 samples from class2. You may use \"np.random.multivariate_normal()\" function to sample data points of multi-variate Gaussian distribution\n",
        "\n",
        "\n",
        "\n",
        "#Case3: Generate 150 samples from class1 and 50 samples from class2. You may use \"np.random.multivariate_normal()\" function to sample data points of multi-variate Gaussian distribution\n",
        "\n",
        "\n",
        "\n",
        "##########################################\n",
        "#Part-4a)-4: Plot the decision regions for all the three cases\n",
        "##########################################\n",
        "#Complete the below function\n",
        "def decisionBoundary(X_train, Y_train):\n",
        "    #Inputs: Training data and Training labels\n",
        "    #Outputs: Decision boundary parameters, i.e., w0, w\n",
        "    p_c1 =\n",
        "    p_c2 = 1. - p_c1\n",
        "\n",
        "    mean_1 =\n",
        "    mean_2 =\n",
        "\n",
        "    cov1 =\n",
        "    cov2 =\n",
        "\n",
        "    cov =\n",
        "\n",
        "    w =\n",
        "    w0 =\n",
        "\n",
        "    return np.array([w0]+list(w))\n",
        "\n",
        "#Plot the decision regions\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x7iZjplMeAP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################\n",
        "#Part-4b)-0: Data Generation\n",
        "##########################################\n",
        "m1 =\n",
        "m2 =\n",
        "cov1 =\n",
        "cov2 =\n",
        "\n",
        "#Case1: Generate 100 samples from each class. You may use \"np.random.multivariate_normal()\" function to sample data points of multi-variate Gaussian distribution\n",
        "\n",
        "\n",
        "\n",
        "#Case2: Generate 50 samples from class1 and 150 samples from class2. You may use \"np.random.multivariate_normal()\" function to sample data points of multi-variate Gaussian distribution\n",
        "\n",
        "\n",
        "\n",
        "#Case3: Generate 150 samples from class1 and 50 samples from class2. You may use \"np.random.multivariate_normal()\" function to sample data points of multi-variate Gaussian distribution\n",
        "\n",
        "\n",
        "\n",
        "##########################################\n",
        "#Part-4b)-4: Plot the decision regions for all the three cases\n",
        "##########################################\n",
        "#Complete the below function\n",
        "def decisionBoundary(X_train, Y_train, X_test):\n",
        "    #Inputs: Training data and Training labels\n",
        "    #Outputs: Decision boundary parameters, i.e., w0, w\n",
        "    p_c1 =\n",
        "    p_c2 = 1. - p_c1\n",
        "\n",
        "    mean_1 =\n",
        "    mean_2 =\n",
        "\n",
        "    cov1 =\n",
        "    cov2 =\n",
        "\n",
        "    #Compute predictions on test data\n",
        "\n",
        "    return predictions\n",
        "\n",
        "#Plot the decision regions\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wLCbJO0EtGOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Report your observations </b>\n",
        "\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "3.\n",
        "\n"
      ],
      "metadata": {
        "id": "Gy6qexxLtLYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Part-5) Classification using logistic regression ( Iterative reweighted least squares approach ): </b> logistic regression model: $y = \\frac{1}{1+e^{-\\textbf{w}^{T}\\textbf{x}}}$. With the provided training data, X_train and Y_train, your task is to identify the optimal $\\textbf{w}$ that accurately predicts y based on the input $\\textbf{x}$. Subsequently, this $\\textbf{w}$ can be applied to predict outcomes on the test data, X_test.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "a). Data generation: Consider a classification scenario with two classes. Class 1 follows a Gaussian distribution with a mean vector $\\begin{bmatrix} 1  \\\\ 1  \\end{bmatrix}$ and a covariance matrix $\\begin{bmatrix} 0.3 & 0.0  \\\\ 0.0 & 0.3 \\end{bmatrix}$. Class 2 follows a Gaussian distribution with a mean vector $\\begin{bmatrix} 2  \\\\ 2  \\end{bmatrix}$ and a covariance matrix $\\begin{bmatrix} 0.3 & 0.0  \\\\ 0.0 & 0.3 \\end{bmatrix}$. Employ these class distributions to generate 50 samples per class for training and 200 samples for testing.\n",
        "\n",
        "b). Create a function called \"Logistic_Regression\" that accepts training data, training labels, and testing data as inputs. The function should aim to discover the optimal $\\textbf{w}$ using the training data. You can initialize $\\textbf{w}$ with random values and iteratively update it to determine the optimal $\\textbf{w}$. Subsequently, this optimal $\\textbf{w}$ can be employed to make predictions on the test data.\n",
        "\n",
        "c). Evaluate the classification performance, i.e., compute the accuracy on the test data.\n",
        "\n",
        "d). Write a function to generate & visualize the decision regions, either by showing the boundary line or by using different colors for the two regions. Overlay the test points using scatter.\n",
        "\n",
        "e). Introduce genuine outliers to one of the class datasets generated previously. Then, conduct Least Squares classification and Logistic Regression on this modified dataset. Plot the decision boundaries/regions and provide insights into the differences between their outcomes.\n",
        "\n",
        "\n",
        "f). Report your observations\n"
      ],
      "metadata": {
        "id": "YmCNOFUVBeCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################\n",
        "#Part-5)-1: Data generation\n",
        "##################################################\n",
        "\n",
        "#Set the parameters of class distribution\n",
        "mean1 =            #Mean of class1\n",
        "mean2 =            #Mean of class2\n",
        "cov =  #Covariance matrix. Same covariance for class1 and class2\n",
        "\n",
        "#Generate training data. You can use \"np.random.multivariate_normal()\" function to sample data points from multivariate Gaussian distribution\n",
        "class1_tr =             #Class1 training data\n",
        "class2_tr =             #Class2 training data\n",
        "tr_data =               #Combine class1 training and class2 training data to get total training data\n",
        "tr_targets =            #Class labels. Assign the value \"0\" to class1 and the value \"1\" to class2.\n",
        "\n",
        "#Generate testing data.\n",
        "class2_te =             #Class1 testing data\n",
        "class2_te =             #Class2 testing data\n",
        "te_data =               #Combine class1 testing and class2 testing data to get total testing data\n",
        "te_targets =            #Class labels. Assign the value \"0\" to class1 and the value \"1\" to class2.\n",
        "\n",
        "\n",
        "##################################################\n",
        "#Part-5)-b: Complete the below logistic regression function\n",
        "##################################################\n",
        "def Logistic_Regression(X_train, Y_train, X_test):\n",
        "  #Inputs: Training data, Training labels, and Testing data\n",
        "  #Outputs: Predictions on test data\n",
        "\n",
        "  max_iter =  #max number of ierations for parameter update\n",
        "  threshold = #threshold for classification\n",
        "  X =          #Data. Append ones to the training data to take care of the bias\n",
        "  w =          #Initialize the weight vector\n",
        "  for iter in range(0,max_iter,1):\n",
        "    preds =    #Predictions on X. Sigmoid(w^T.x)\n",
        "    error =    #Difference between preds and Y_train\n",
        "\n",
        "    #Compute the gradients and the appropriate learning rate. We can use Hessian matrix as learning rate\n",
        "\n",
        "\n",
        "    #Update the weight vector\n",
        "\n",
        "\n",
        "  #Test data predictions\n",
        "\n",
        "\n",
        "\n",
        "  #Compare the predictions with 0, to output the decision\n",
        "\n",
        "\n",
        "\n",
        "  #Return the predicted test labels\n",
        "  return test_preds\n",
        "\n",
        "\n",
        "Y_pred = Logistic_Regression(tr_data, tr_targets, te_data)\n",
        "\n",
        "###############################################\n",
        "#Part-5)-c:\n",
        "###############################################\n",
        "def LR_Accuracy(Y_test, Y_pred):\n",
        "  #Inputs: Ground truth and predicted lables of test data\n",
        "  #Outputs: Accuracy\n",
        "\n",
        "  #Compute and return the accuracy\n",
        "  return acc\n",
        "\n",
        "\n",
        "acc = LR_Accuracy(te_targets, Y_pred)\n",
        "print('\\n\\n Accuracy with Logistic Regression:', acc, '\\n\\n')\n",
        "\n",
        "\n",
        "##########################################\n",
        "#Part-5)-d: Plot the decision regions. Overlay the test points on the plot\n",
        "##########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########################################\n",
        "#Part-5)-e: Logistic regression for the data with true outliers\n",
        "##########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sRnm56O0BeCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Part (6) : Estimating the parameters of Gaussian Mixture Models (GMM) using Expectation-Maximization (EM) algorithm </b>\n",
        "\n",
        "\n",
        "<dt> <b> a. Generating synthetic data : </b>  This part describes the steps involved in generating the synthetic data. </dt>\n",
        "\n",
        "\n",
        "<dt>  1. Let us consider the GMM with Bivariate Gaussians   </dt>\n",
        "<dd>  - Choose the appropriate means ($\\mu$), covariances ($\\Sigma$), and weights ($\\pi$) of Bivariate Gaussian distributions. You may choose the following parameters for this asignment.  </dd>\n",
        "<br>\n",
        "<dt>  <center> $ \\pi = \\begin{bmatrix} 0.6  \\\\ 0.4 \\end{bmatrix} \\;\\;\\;\\; \\mu_{1} = \\begin{bmatrix} -1.0  \\\\ -1.0 \\end{bmatrix} \\;\\;\\;\\; \\mu_{2} = \\begin{bmatrix} 1.0  \\\\ 1.0 \\end{bmatrix} \\;\\;\\;\\; $ </center>   </dd>\n",
        "<br>\n",
        "<dt>  <center> $ \\Sigma_{1}^{full} = \\begin{bmatrix} 0.15 & 0.1  \\\\ 0.1 & 0.25 \\end{bmatrix} \\;\\;\\;\\; \\Sigma_{2}^{full} = \\begin{bmatrix} 0.3 & -0.25  \\\\ -0.25 & 0.25 \\end{bmatrix}  $ </center>  </dd>\n",
        "<br>\n",
        "<dt>  <center> $ \\Sigma_{1}^{diagonal} = \\begin{bmatrix} 0.1 & 0.0  \\\\ 0.0 & 0.2 \\end{bmatrix} \\;\\;\\;\\; \\Sigma_{2}^{diagonal} = \\begin{bmatrix} 0.2 & 0.0  \\\\ 0.0 & 0.1 \\end{bmatrix}  $ </center>  </dd>\n",
        "<br>\n",
        "<dt>  <center> $ \\Sigma_{1}^{spherical} = \\begin{bmatrix} 0.2 & 0.0  \\\\ 0.0 & 0.2 \\end{bmatrix} \\;\\;\\;\\; \\Sigma_{2}^{spherical} = \\begin{bmatrix} 0.1 & 0.0  \\\\ 0.0 & 0.1 \\end{bmatrix}  $ </center>  </dd>\n",
        "\n",
        "<dd>  - Pick one Gaussian following the selection probability as  $\\pi$    </dd>\n",
        "<dd>  - Sample the data point from the selected Bivariate Gaussian distribution    </dd>\n",
        "<dd>  - Repeat the process N times to get the entire data set  </dd>\n",
        "<dt>  2.Create the synthetic data set with the number of mixtures as 2.  </dt>\n",
        "<dt>  3.Plot the synthetic data set with unique colors to the data points drawn from each Bivariate Gaussian distribution in GMM  </h4> </dt> </dd>\n",
        "\n",
        "<dt> <b> b. Implementation of Expectation and Maximization (EM) algorithm : </b> This part focuses on implementing the EM algorithm from scratch to estimate the parameters of the Gaussian Mixture Model (GMM). </dt>  \n",
        "\n",
        "<dt>  1. Chose the number of mixtures (M) and initialize the parameters of GMM model  </dt>\n",
        "<dd>  - Make sure that the diagonal elements of covariance are positive, and the weights of Gaussians are positive and sum to 1 for satisfying the axioms of probability.  </dd>\n",
        "\n",
        "<dt>   2. Expectation step :  Evaluate the responsibility of Gaussians in generating the data points using the current estimated parameters i.e $\\hat\\mu_{k}$ , $\\hat\\Sigma_{k}$ and $\\hat\\pi_{k}$, here k indicates the Gaussian in GMM  </dt>\n",
        "<br>\n",
        "\n",
        "<dd>  <center> $\\gamma_{nk} = \\frac{\\pi_{k} N ( \\textbf{x}_{n} / \\mu_{k} , \\Sigma_{k} )}{ \\sum_{m=1}^{M} \\pi_{m} N ( x_{n} / \\mu_{m} , \\Sigma_{m} ) }    \\;\\;\\;\\;\\;\\; ∀ \\;\\;k=1,2,...,M \\;\\; and \\;\\; n=1,2,...,N$  </center>  </dd>\n",
        "\n",
        "<br>\n",
        "\n",
        "<dd>   Where N is the total number of data points and M is the total number of Gaussians in GMM   </dd>\n",
        "\n",
        "<dd>   - $\\gamma_{nk}$ represents the responsibility of $k^{th}$ Gaussian in generating the $n^{th}$ data point   </dd>\n",
        "\n",
        "<dd>   - $\\sum_{n=1}^{N}$ $\\gamma_{nk}$ represents the the effective number of data points generated by $k^{th}$ Gaussian   </dd>\n",
        "\n",
        "\n",
        "<dt>   3. Maximization step :  Update the parameters ($\\hat\\mu_{k}$ , $\\hat\\Sigma_{k}$ and $\\hat\\pi_{k}$) of GMM using the current resosibilities ($\\gamma_{nk}$) by maximizing the likelihood. </dt>\n",
        "\n",
        "<dd>\n",
        " - Effective number of data points generated by $k^{th}$ Gaussian : $N_{k}$ = $\\sum_{n=1}^{N}$ $\\gamma_{nk}$   \n",
        " </dd>\n",
        "\n",
        "<dd>\n",
        " - Mean of $k^{th}$ Gaussian : $\\hat{\\mu_{k}} = \\frac{1}{N_{k}} \\sum_{n=1}^{N}\\gamma_{nk} x_{n}$   \n",
        " </dd>\n",
        "\n",
        "<dd>\n",
        " - Covariance of $k^{th}$ Gaussian : $\\hat{\\Sigma_{k}} = \\frac{1}{N_{k}} \\sum_{n=1}^{N} \\gamma_{nk} ( x_{n} - \\hat{\\mu_{k}} )( x_{n} - \\hat{\\mu_{k}} )^{T}$   \n",
        " </dd>\n",
        "\n",
        "<dd>\n",
        " - Weight of $k^{th}$ Gaussian : $\\hat{\\pi_{k}} = \\frac{N_{k}}{N}$  \n",
        " </dd>\n",
        "\n",
        "<dt>  4. Compute the log-likelihood with the updated parameters  </dt>\n",
        "<dt>   5. Repeat the expectation and maximization steps untill convergence  </dt>\n",
        "\n",
        "<dt>  <b> c. Illustration of EM iterations </b>  </dt>\n",
        "\n",
        "<dt>   5. Plot the contours of the estimated distributions over the iterations in the EM algorithm and overlay the data points.  </dt>\n",
        "\n",
        "<dt>  <b> d. Understanding the issues with EM algorithm </b>  </dt>\n",
        "<dt>   6. Check the influence of initialization on the convergence of the EM algorithm and fit quality.  </dt>\n",
        "<dd>   - Demonstrate the observations with the plots mentioned in the \"Illustration of EM iterations\" section    </dd>\n",
        "<dt>   6. Check the influence of the type of covariance matrix on the convergence of the EM algorithm and fit quality . </dt>\n",
        "<dd>   - Execute the EM algorithm with different covariance matrices i.e Spherical, Diagonal and Full covariance  </dd>\n",
        "<dd>   - Demonstrate the observations with the plots mentioned in the \"Illustration of EM iterations\" section  </dd>\n"
      ],
      "metadata": {
        "id": "Ez4oAqxUd85P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#All imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rand\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from scipy.stats import norm, multivariate_normal\n",
        "import sys\n",
        "import math\n",
        "\n",
        "##########################################\n",
        "#Part-6)-a: Generating synthetic data\n",
        "##########################################\n",
        "\n",
        "def generateData(pi, mu, sigma, N):\n",
        "    ############################################################################\n",
        "    #pi = weights of Gaussians, mu = mean of Gaussians, sigma = convariance of Gaussians, N = number of data points\n",
        "    #Steps\n",
        "    #1.Select th Gaussian following the probability distribution as pi\n",
        "    #2.Sample the data points from the selected Gaussian\n",
        "    #3.Repeat the process N times to get N data points\n",
        "    #4.Return the sampled data\n",
        "    ############################################################################\n",
        "    #You may look into \"random.choices\" and \"np.random.multivariate_normal\" functions\n",
        "\n",
        "\n",
        "\n",
        "    return np.asarray(data), np.asarray(labels)\n",
        "    ############################################################################\n",
        "\n",
        "\n",
        "############################################################################\n",
        "#Chose the appropriate means ($\\mu$), covariances ($\\Sigma$), and weights\n",
        "############################################################################\n",
        "mu1 =    #Mean vector of component1 in GMM\n",
        "sig1 =   #Full covariance matrix of component1 in GMM\n",
        "mu2 =    #Mean vector of component2 in GMM\n",
        "sig2 =   #Full covariance matrix of component2 in GMM\n",
        "pi =     #Prior probabilities\n",
        "\n",
        "n_points =  #Number of points\n",
        "\n",
        "# Spherical covariances\n",
        "sig1_s =\n",
        "sig2_s =\n",
        "# Diagonal covariances\n",
        "sig1_d =\n",
        "sig2_d =\n",
        "\n",
        "############################################################################\n",
        "#Create the synthetic data set\n",
        "############################################################################\n",
        "\n",
        "data_full, labels_full = generateData(pi, [mu1, mu2], [sig1, sig2], 200)\n",
        "data_sp, labels_sp = generateData(pi, [mu1, mu2], [sig1_s, sig2_s], 200)\n",
        "data_dg, labels_dg = generateData(pi, [mu1, mu2], [sig1_d, sig2_d], 200)\n",
        "\n",
        "############################################################################\n",
        "#Plot the synthetic data set\n",
        "############################################################################\n",
        "\n",
        "fig, ax = plt.subplots(1,3, figsize=(20,7))\n",
        "\n",
        "#Visualize the data sampled from the GMM with full covariance matrix\n",
        "ax[0].set_title('Full Covariance')\n",
        "\n",
        "#Visualize the data sampled from the GMM with spherical covariance matrix\n",
        "ax[1].set_title('Spherical Covariance')\n",
        "\n",
        "#Visualize the data sampled from the GMM with diagonal covariance matrix\n",
        "ax[2].set_title('Diagonal Covariance')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AYWUw6FIi91a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#All imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rand\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm, multivariate_normal\n",
        "import sys\n",
        "import math\n",
        "\n",
        "##########################################\n",
        "#Part-6)-b: Implimentation of Expectation-Maximization (EM) algorithm\n",
        "##########################################\n",
        "\n",
        "############################################################################\n",
        "#Write a function to compute the log-likelihood of the data given the parameters\n",
        "############################################################################\n",
        "def likelihood(data, pi, mu, sigma, log_likelihood=False, likelihoods=False):\n",
        "  #Inputs: data, parameters of GMM model\n",
        "  #Outputs: likelihood\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################################################################\n",
        "#Write a function for the expectation step\n",
        "############################################################################\n",
        "def expectation(data, pi, mu, sigma):\n",
        "  #Inputs: data, parameters of GMM model\n",
        "  #Outputs: gamma\n",
        "  gamma =\n",
        "  for n in range(len(data)):\n",
        "    #Iterate through the data points\n",
        "    for k in range(len(pi)):\n",
        "      #Iterate through the components in GMM\n",
        "      numerator =\n",
        "      denominator =\n",
        "      gamma[n,k] = numerator/denominator\n",
        "  return gamma\n",
        "\n",
        "\n",
        "\n",
        "############################################################################\n",
        "#Write a function for the maximization step.\n",
        "############################################################################\n",
        "def maximization(data, gamma):\n",
        "  #Inputs: data, gamma\n",
        "  #Outputs: Updated parameters of GMM model\n",
        "\n",
        "  #Update mu\n",
        "\n",
        "\n",
        "  #Update sigma\n",
        "\n",
        "\n",
        "  #Update pi\n",
        "\n",
        "\n",
        "\n",
        "  #Return the parameters of GMM model\n",
        "  return pi, mu, sigma\n",
        "\n",
        "\n",
        "#Complete the below function. It plots the contours\n",
        "def plot_contours(data, labels, pi, mu, sigma):\n",
        "\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "############################################################################\n",
        "#Run the expectation and maximzation algorithm to estimate the parameters\n",
        "############################################################################\n",
        "data =     #data to run EM-algorithm\n",
        "labels =   #labels\n",
        "N =        #number of input examples\n",
        "D =        #data dimension\n",
        "M =        #no.of components in GMM\n",
        "\n",
        "pi =       #Initialize the prior probabilities\n",
        "mu =       #Initialize the mean vectors\n",
        "sigma =    #Initialize the covariance matrices\n",
        "\n",
        "\n",
        "nsteps =   #Number of steps to run EM algorithm\n",
        "for index in range(0,nsteps,1):\n",
        "\n",
        "  #Compute the likelihood\n",
        "  ll = likelihood(data, pi, mu, sigma, log_likelihood=True)\n",
        "\n",
        "  #Plot the contours\n",
        "  plot_contours(data, labels, pi, mu, sigma)\n",
        "\n",
        "  #Expectation step\n",
        "  gamma = expectation(data, pi, mu, sigma)\n",
        "\n",
        "  #Maximization step\n",
        "  pi, mu, sigma = maximization(data, gamma)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0EZhq9phk38Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################################\n",
        "##Part-6)-d: Understand the issues with EM algorithm\n",
        "############################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-XbWJoMVmyvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Report your observations </b>\n",
        "\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "3.\n",
        "\n"
      ],
      "metadata": {
        "id": "6o6d-jlmm-uD"
      }
    }
  ]
}